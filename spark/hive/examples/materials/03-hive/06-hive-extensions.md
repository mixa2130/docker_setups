## 0. Оболочка HUE

* Проброс порта: `ssh <USERNAME>@sber-client.atp-fivt.org -L 8888:sber-node01:8888`
* Логин такой же как на кластере, только заменяем 2022 на 2021
* Пароль такой же как **логин** на кластере, только заменяем 2022 на 2021 (желательно сменить при первом входе).

## 1. Hive streaming (или ["Hive transform"](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Transform))
Существует 2 основных способа использования внешних скриптов в Hive Streaming.
* использование команды,
* подключение внешних скриптов.

> **Пример.** Вывести 1-й октет IP-адресов из таблицы Subnets.

Решение 2-мя способами: `7-streaming_example`

 - `TRANSFORM`: выбирает поле, кот. мы будем обрабатывать с помощью streaming.
 - `USING`: команда или подключаемый скрипт, кот. обрабатывает поле.
 - `AS`: alias'ы для полей, кот. получаются после обработки стримингом. Полей модет быть несколько.

Внешние скрипты (по аналогии с Hadoop Streaming) не забываем добавлять в Distributed Cache с помощью `ADD FILE`.

###### 1.1. Отладка скриптов для Streaming
Внешние скрипты могут быть достаточно сложными, поэтому удобно сначала их отладить.
``` bash
hive --database <YOUR_USER>_test -e 'SELECT * FROM SerDeExample LIMIT 10' | ./<your_script>.sh
```
 
###### 1.2. Практика
Входные данные - таблица логов (SerDeExample).
> **Задача 4.** Заменить в логах дату 20140101 на сегодняшнее число, используя Streaming.

Входные данные - таблица логов (SerDeExample) или таблица подсетей (Subnets)
> **Задача 5.** Перевести IP-адреса в численное представление. Для быстрого перевода можно воспользоваться таким Python-кодом: `struct.unpack("!I", socket.inet_aton(ip)`

> По каждой задаче выведите план запроса и посчитайте по нему кол-во MapReduce Job.

## 2. Пользовательские функции

* Regular UDF: обрабатываем вход построчно,
* UDAF: аггрегация, n строк на вход, 1 на выходе,
* UDTF: 1 строка на вход, таблица (несколько строк и полей) на выходе,
* Window functions: "окно" (несколько строк, *m*) на вход, несколько строк(*n*) на выходе (1 строка для каждого окна). Функции аггрегации и UDAF тоже могут быть использованы в качестве оконных.

#### 2.1. (Regular) User-defined functions

1. Для реализации UDF нужно создать Java-класс, являющийся наследником класса org.apache.hadoop.hive.ql.exec.UDF.
2. Реализовать в этом классе один или несколько методов evaluate(), в которых будет записана логика UDF.
3. Для сборки нужно подключить ещё один Jar-файл:
```
/opt/cloudera/parcels/CDH/lib/hive/lib/hive-exec.jar
```
4. Для использования UDF в запросе нужно:

  а) добавить собранный Jar-файл в Distributed cache (можно использовать относительный путь):
```
ADD JAR <path_to_jar>
```
При этом никаких дополнительных Jar-файлов в запросе можно не добавлять т.к. Jar с UDF уже содержит все необходимые коды.

  б) создать функцию на основе Java-класса UDF:
```
CREATE TEMPORARY FUNCTION <your_udf> AS 'com.your.OwnUDF';
```
> **Пример 10.** Реализовать UDF, которая возвращает тоже, что было подано ей на вход без каких-либо изменений.

На данном примере можно изучить синтаксис UDF и использовать его в дальнейших задачах. Код UDF и запроса с её использованием лежит в:
```
1-example_udf[example.sql, Identity/]
```

> **Задача 11.** Реализовать UDF, принимающую на вход IP-адрес. На выход UDF выдаёт число - сумму октетов адреса. Можно использовать как таблицу Subnets, так и SerDeExample т.к. IP есть в обеих.

С UDF:
* много кода,
* только **Java** :(

Без UDF:
* Ещё больше кода (правда на SQL). Пример: `2-sum_udf/query_without_udf.sql`
* Не всегда можно реализовать в 1 запрос => будут подзапросы => будет несколько Job (дольше).

#### 2.2. User-defined table functions (UDTF)

От обычных UDF данный вид функций отличается тем, что на выходе может быть больше одной записи. Причём столбцов также может быть сгенерировано несколько, т.е. по одной записи на входе мы можем получить целую таблицу. Отсюда и название.

1. Для реализации UDTF нужно создать класс-наследника от org.apache.hadoop.hive.ql.udf.generic.GenericUDTF.
2. Логика UDTF пишется в 3 методах:

   а) `initialize()`: 
     - разбор входных данных (проверка количества аргументов и их типов), сохранение данных в ObjectInspector'ы
     - создание структуры выходной таблицы (названия и типы полей)
     
   б) `process()`: реализация механизма получения выходных данных из входных,
   
   в) `close()`: аналог cleanup() в MapReduce. Обрабатывает то, что не было обработано в `process()`. Здесь не используется т.к. не аггрегируем, используется в основном в UDAF, см. ниже.
  
3. Собираем Jar также, как и в случае с обычными UDF, однако для сборки подключить нужно не 1, а 2 дополнительных Jar:
```
/opt/cloudera/parcels/CDH/lib/hive/lib/hive-exec.jar
/opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar
```

> **Пример 12.** Реализовать UDTF, принимающую на вход IP-адрес. На выход выдаём этот же адрес, повторённый дважды.  Чтоб разграничить выводы для каждого IP, последней строкой в столбце выведите разделитель "-----".

|Вход|Выход|
|:----:|:---:|
|60.143.233.0|60.143.233.0|
||60.143.233.0|
||-----|
|14.226.82.0|14.226.82.0|
||14.226.82.0|
||-----|

Код UDTF и запроса с её использованием лежит в:
```
3-example_udtf[example.sql, CopyIp/]
```
> **Задача 13.** Реализовать UDTF, принимающую на вход IP-адрес. На выход выдаём список октетов адреса. Чтоб разграничить выводы для каждого IP, последней строкой в столбце выведите числовой разделитель (исп. Integer.MAX_VALUE).

|Вход|Выход|
|:----:|:---:|
|60.143.233.0|60|
||143|
||233|
||0|
||2147483647|
|14.226.82.11|14|
||226|
||82|
||11|
||2147483647|

Код UDTF и запроса с её использованием лежит в:
```
4-octets_udtf
```

#### 2.3. User-defined aggregation functions (UDAF)

Позволяют реализовать свои функции наподобие `SUM()`, `COUNT()`, `AVG()`.

**Доп. литература.** [Programming hive](https://www.oreilly.com/library/view/programming-hive/9781449326944/), гл. 13 "Functions" (с. 163). 

### Задача
Напишите запрос, выбирающий количество посещений от мужчин и от женщин по типам браузера (информацию о браузерах берём из таблицы логов).

## 3. Форматы хранения данных в Hive
* [Как использовать Parquet и не поскользнуться](https://habr.com/ru/company/wrike/blog/279797/)
* [Сравнение форматов сериализации](https://habr.com/ru/post/458026/)

## 4. Несбалансированные данные 

* [SKEWED BY при кластеризации](https://cwiki.apache.org/confluence/display/Hive/ListBucketing#ListBucketing-SkewedTablevs.ListBucketingTable)

# Apache Spark

* [Инструкция по работе на нашем кластере](https://docs.google.com/document/d/1cIEVOdSb_v086EsQyCEwXmo3tmL1mymnZLGbfMwEQ68/edit). Все материалы в Jupyter.
