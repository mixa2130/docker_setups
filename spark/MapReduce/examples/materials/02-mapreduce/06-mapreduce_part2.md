# Hadoop Java API

Исходник: `...materials/02-mapreduce/05_wordcount_java`.

#### Основной класс

`public class WordCount extends Configured implements Tool {...}`

* Класс WordCount содержит всю логику задачи
* Базовый класс [Configured](https://hadoop.apache.org/docs/r2.4.1/api/org/apache/hadoop/conf/Configured.html) отвечает за возможность получить конфигурацию HDFS (достаточно вызвать `getConf()` внутри `run()`). Это полезно в тех случаях, когда нужно работать с HDFS из программы (например, удалять промежуточные результаты),
* [Tool](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/util/Tool.html) - интерфейс, содержащий единственный метод `run()`, который (а) парсит аргументы командной строки, (б) производит настройку Job'ы. Выполняется `run()` **на клиенте**.

#### Метод `run()`. Важные моменты.

1. Форматы ввода-вывода:
```java
job1.setInputFormatClass(TextInputFormat.class);
job1.setOutputFormatClass(TextOutputFormat.class);
```
Это форматы, в которых пишется результат Job'ы. В Hadoop есть несколько встроенных форматов ввода-вывода. Основные:
**TextInputFormat / TextOutputFormat**
Результат пишется в стандартный текстовый файл.
Плюсы: быстро работает.
Минусы: теряет данные о типах ключей-значений. При считывании данных будем иметь пары `(LongWritable offset, Text rawString)`. rawString нужно парсить.

**KeyValueTextInputFormat / KeyValueTextOutputFormat**
Улучшенная версия предыдущего. Типы ключей-значений хранит только если это простые типы (например, IntWritable, Text).

**SequenceFileInputFormat / SequenceFileTextOutputFormat**
Плюсы: хранит типы ключей-значений.
Минусы: Выходной файл будет записан в формате бинарного файла и прочитать его командой `hdfs dfs -cat` не выйдет.

В итоге:
* в промежуточных job'ах лучше использовать форматы KeyValue или SequenceFile.
* В последней job'е - TextOutputFormat

Любой Input / OutputFormat позволяет задавать сжатие. Это сэкономит затраты на передачу по сети.

```java
SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);
SequenceFileOutputFormat.setCompressOutput(job, true);
```

CompressionType - механизм сжатия. Бывает `BLOCK` (сжимаем поблочно) и `RECORD` (сжимаем записи раздельно).

*CompressionType:  Тем, кто слушает Java, полезно посмотреть на хорошую реализацию `enum` с полями и методами.*

[Больше](http://timepasstechies.com/input-formats-output-formats-hadoop-mapreduce/) входных и выходных форматов.

2. Задание кол-ва мапперов и редьюсеров.

`job.setNumReduceTasks(8);` - задаем кол-во редьюсеров.
Число мапперов напрямую задать нельзя. Система устанавливает его равным кол-ву сплитов.
* Размер сплита можно изменять (по умолчанию равен размеру блока)
* В логах Hadoop-задачи можно найти строчку: `19/10/14 21:37:44 INFO mapreduce.JobSubmitter: number of splits:2`.

#### Мапперы и редьюсеры

Соoтветствуют паттерну, см. [выше](/distribute/practice/03-hadoop2.md#%D1%81%D1%82%D1%80%D1%83%D0%BA%D1%82%D1%83%D1%80%D0%B0-mapreduce-task-map-reduce). В отличие от Hadoop Streaming, цикл обхода сплита писать не надо.
Выполняются на нодах.

#### Классы-обёртки
В Hadoop существует 2 интерфейса: `Writable` и `WritableComparable`. Оба они поддерживают сериализацию / десериализацию, нужную для передачи данных между нодами.
* Типы входных-выходных ключей должны реализовывать интерфейс `WritableComparable` (т.к. их придётся сравнивать на этапе сортировки).
* Для значений достаточно `Writable`.
* Классы-обёртки, входящие в поставку Hadoop, реализовывают интерфейс `WritableComparable`.

#### Сборка и запуск программмы
Собирать Java-проект можно с помощью ant или maven. См. соответственно build.xml и pom.xml в `...materials/02-mapreduce/05_wordcount_java`.
Запуск программы: `hadoop jar <путь_к_jar> <полное_имя_главного_класса> <вход> <выход> [другие_аргументы (напр. кол-во редьюсеров)]`

### Задача
1. Добавим combiner в WordCount. Для этого в настройках job'ы (`run()`) добавляем метод `setCombinerClass`. Подходит ли редьюсер на роль combiner'a? Сравним быстродействие работы программы с combiner и без.
2. Внесём изменения в WordCount:
   * не учитывать слова короче 4 символов (см. стоп-слова).
   * выводить в результат только те слова, у которых кол-во встречаемости > 4. 

## Глобальная сортировка
Внутри выдачи редьюсера сортировка соблюдается. Как сделать общую сортировку для всех редьюсеров? 
* Лобовое решение: пройтись по всем ключам и явно сказать Partitioner'у, какие ключи попадут на тот или иной редьюсер. Это долго.
* Выход - семплирование. Проходим не все ключи, а выбираем с некоторой вероятностью, затем аппроксиимруем на весь датасет.

Пример кода: `InputSampler.Sampler<LongWritable, Text> sampler = new InputSampler.RandomSampler<>(0.5, 10000, 10);`. 
* 0,5 - вероятность выбора записи:
* максимальное кол-во "выборов"
* максимальное кол-во сплитов.

Как только дошли до границы хотя бы по одному аргументу, семплирование прекращаем.

Такой Sampler подаётся в TotalOrderPartitioner. Подробнее см. "Hadoop. The definitive guide, 4 изд. стр. 287".
Пример WordCount с глобальной сортировкой: `...materials/02-mapreduce/10-globalsort`.

### Задача
Поэкспериментируем с параметрами InputSampler. Например, уменьшим вероятность до 0,2. Если данных мало, Sampler не сможет сформировать выборку и программа упадёт.

### Joins

По данным stackoverflow посчитайте гистограмму количества вопросов и ответов в зависимости по возрастам пользователей. В случае отсутствия или невалидного возраста пусть будет 0. Выведите ее на печать, сортировка по возрасту (числовая, по возрастанию).
На печать: весь результат, сортировка по возрасту.

* *Входные данные:* посты stackoverflow.com
* *Формат ответа:* age <tab> num_questions <tab> num_answers

##### Описание входных данных.

`/data/stackexchange/posts` - сами посты, записаны в строках, начинающихся с ‘<row’ (можно разбирать строки вручную, без специальных xml-парсеров). Значение полей:

* PostTypeId - ‘1’, если это вопрос; ‘2’, если ответ на вопрос
* Id - идентификатор; если это вопрос, то он откроется тут: http://stackoverflow.com/questions/<Id>
* Score - показатель полезности этого вопроса или ответа
* FavoriteCount - добавления вопроса в избранные
* ParentId - для ответа - идентификатор поста-вопроса
* **OwnerUserId - идентификатор пользователя - автора поста**

`/data/stackexchange/users` - пользователи

* **Id - идентификатор пользователя** (в posts это OwnerUserId)
* Age - возраст (может отсутствовать)
* Reputation - уровень репутации пользователя

Есть несколько семплов датасета (stackexchange100, stackexchange1000 - 100-я и 1000-я часть исходного датасета соответственно).

##### Схема решения: 
*1я Job*
* **Mapper**: строка датасета (users или posts) -> пары таких типов: 
    * (userId, tag, возраст, 0, 0) для пользователя,
    * (userId, tag, 0, 1, 0) для вопроса
    * (userId, tag, 0, 0, 1) для ответа

`tag` разделяет пользователей и посты (например U для пользователей и P для постов).

`userId` это Primary key, по которому будем делать join.

`возраст` храним т.к. именно по нему нужно будет сортировать

Работать со сложными ключами тяжело. В Hadoop Streaming приходится настраивать comparator и partitioner, на Java API удобнее написать свой Writable-класс (с сериализацией). В примере это StackExchangeEntry.java

* **Reducer**: (userId, [(tag, возраст, 0, 0), (tag, 0, 1, 0), (tag, 0, 1, 0), ...]) -> (возраст, кол-во\_вопросов, кол-во\_ответов)

*2я Job*
* Доаггрегирование пар (возраст, кол-во\_вопросов, кол-во\_ответов). WordCount по возрасту (ключ - возраст, значение - вопросы и ответы).
* Сортировка по возрасту.

Решение задачи: `...materials/02-mapreduce/11-joins`.

**СЛОЖНО!!**. Выход: Hive.

### Ещё 1 задача на MapReduce

Оценить среднее время жизни страниц домена в днях. Отсортировать по времени жизни, результат: echo.msk.ru 5 означает, что на сайте "Эхо Москвы" статьи в среднем посещаются 5 дней. Смысл показателя: на новостных сайтах страницы меняются быстро, на материалы глянцевых, автомобильных, и научно-популярных журналов пользователи приходят в течение нескольких недель. Можно проверить гипотезу о том, что среднее время жизни статей на таких сайтах будем разным.

Данные о посещениях пользователями страниц в Сети с отметкой в времени посещения и времени, которое пользователь провел на странице. Период сбора данных - 2 недели. Формат файла данных - текст в tab-separated формате с полями user_id, timestamp, url, 'diff_time':time_on_page.

Например:
```
7884862957065236246 1412508741 http://lenta.ru/articles/2014/10/05/teacher/ 63
```

Тут:

* 7884862957065236246 - обезличенный идентификатор пользователя, 
* 1412508741 - timestamp, как принято в Unix в секундах с 01.01.1970, 
* далее адрес страницы и время на странице в секундах 
* diff_time: - в данном случае ненужный префикс.

Данные в HDFS в директории `/data/user_events` и семпл для отладки в `/data/user_events_part`.

**Опишите схему вычислений на MapReduce.**

Исходник: `...materials/02-mapreduce/12-avg-page-life`.

# Hadoop Streaming advanced
Данные представляют собой статистику по активности пользователей сервера Minecraft. 
Отсортировать по среднему кол-ву команд. При равном среднем количестве команд, отсортировать лексикографически по никам пользователей.
```
Login    ср. кол-во команд на сессию     общее кол-во сессий
```
Путь к данным: `/data/minectaft_user_activity`

##### Пример вывода:
```
lucky20    25.0    10
avivzusim   5.0    15
```

### Решение
1. Указываем то, что в ключе 2 поля
2. Используем KeyFieldBasedComparator
3. В опциях настраиваем сортировку по 2-му полю в обратном (r) порядке и числовую (n) сортировку.
4. При прочих равных сортируем по 1м полю.
Подробнее см. `man sort`.

```bash
#!/usr/bin/env bash

IN_DIR="/data/minectaft_user_activity"
OUT_DIR="minecraft_result"

# Look at the input data
hdfs dfs -cat ${IN_DIR}/part-00000 | head -n 10

# Remove previous results
hdfs dfs -rm -r -skipTrash ${OUT_DIR}* > /dev/null

yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -D stream.num.map.output.key.fields=2 \
    -D mapreduce.job.reduces=1 \
    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \
    -D mapreduce.partition.keycomparator.options='-k2,2nr -k1' \
    -mapper cat \
    -reducer cat \
    -input ${IN_DIR} \
    -output ${OUT_DIR}

# Checking result
hdfs dfs -cat ${OUT_DIR}/part-00000 | head -n 10
```

