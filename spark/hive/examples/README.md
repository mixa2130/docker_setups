## Исходные данные: логи пользователей

Данные находятся в HDFS по адресу `/data/user_logs/*_M`. Они состоят из трёх частей, каждая из которых находится в своей поддиректории. Данные в каждой части отличаются количеством и типом колонок, разделенных знаками табуляции ('\t') или пробелами.

#### А. Логи запросов пользователей к новостным сообщениям (user_logs).
1. Ip-адрес, с которого пришел запрос (STRING),
2. Время запроса (TIMESTAMP или INT),
3. Пришедший с ip-адреса http-запрос (STRING),
4. Размер переданной клиенту страницы (INT),
5. Http-статус код (INT).
6. Информация о клиентском приложении, с которого осуществлялся запрос на сервер, в том числе, информация о браузере (STRING).

**Важно:** информация о браузере содержится в начале 6-ого поля лога (символы с нулевой позиции до позиции первого пробельного символа), содержание оставшейся части строки не определяет браузер пользователя. Разделитель между IP и временем запроса имеет 3 табуляции.

#### B. Информация о пользователях (user_data).
1. IP-адрес (STRING),
2. Браузер пользователя (STRING),
3. Пол (STRING) //male, female,
4. Возраст (INT).

#### С. Информация о местонахождении IP адресов пользователей (ip_data).
1. IP-адрес (STRING),
2. Регион (STRING).


## Задачи

**Задача 1 (411)**. Создайте внешние (EXTERNAL) таблицы по исходным данным. В результате будет 4 таблицы: логи пользователей, данные ip адресов, данные пользователей и подсети. Из таблицы логов перенесите данные в другую таблицу, партицированную по датам – одна партиция на каждый день. На партиционированных таблицах и нужно будет выполнять запросы в следующих задачах.

Требуется, чтобы сериализация и десериализация данных осуществлялась с использованием регулярных выражений (см. `org.apache.hadoop.hive.serde2.RegexSerDe`).

Проверить правильность создания таблиц с помощью простейших запросов (`SELECT * FROM <table> LIMIT 10`). Эти Select запросы нужно также добавлять в скрипт задачи.

*Пример результата:*
```
33.49.147.163	http://lenta.ru/4303000	1189	451	Chrome/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0)n	20140101
75.208.40.166	http://newsru.com/3330815	60	306	Safari/5.0 (Windows; U; MSIE 9.0; Windows NT 8.1; Trident/5.0; .NET4.0E; en-AU)n	20140101
```

**Задача 2.**. Напишите запрос, выбирающий количество посещений для каждого дня. Полученные результаты отсортируйте по убыванию количества.

*Пример результата:*
```
20140308	96
20140409	96
20140318	96
```
Т.к. после агрегации данных становится немного, `LIMIT` использовать не надо.

**Задача 3.**. Напишите запрос, выбирающий количество посещений от мужчин и от женщин по регионам.

*Пример результата:*
```
Tver	66968157	29097223
Voronezh	60445347	26333509
```

**Задача 4**. Представьте ситуацию, что все новостные сайты переехали в домен .com. Вас попросили обновить базу логов, чтоб логи пользователей указывали не на старые домены, а на новые. Например, новостная ссылка http://news.rambler.ru/8744806 теперь должна выглядеть в ваших запросах как http://news.rambler.com/8744806. Используйте стриминг в hive-sql запросе. (Рекомендуется обратить внимание на команды awk и sed). Выведите TOP-10 записей логов без сортировки.

*Пример результата:*
```
49.203.96.67	20140102	http://lenta.com/2296722	716	499	Safari/5.0
33.49.147.163	20140102	http://news.yandex.com/5605690	850	300	Safari/5.0
```
Задача 4 должна быть решена с использованием Hive Streaming. Заметьте, что в выводе требуется не только то поле, которое мы изменяем, а все 6 полей.

### Комментарии
1. Если Hive падает с ошибкой:

```java
ERROR Unable to invoke factory method in class class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender for element HushableMutableRandomAccess. java.lang.reflect.InvocationTargetException
```
, отключите автоматический MapJoin. Это делается так: 

```
SET hive.auto.convert.join = false;
```

6. По умолчанию максимальное кол-во партиций в Hive 100, но в таблице Logs уникальных ключей больше 100 поэтому при запуске партиционирования появляется ошибка "Fatal error ocurred when node tried to created to many dynamic partitions". Увеличить кол-во партиций можно так:
```
SET hive.exec.max.dynamic.partitions=116;
SET hive.exec.max.dynamic.partitions.pernode=116;
```