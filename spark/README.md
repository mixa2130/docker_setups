## Installation

docker images

~~~bash
docker pull itayb/spark:3.1.1-hadoop-3.2.0-aws
docker pull itayb/jupyter-notebook:6.2.0-spark-3.1.1-java-11-hadoop-3.2.0
~~~

## Конспекты:

### DWH

**DWH**(Data WareHouse) - хранилище разных данных, которые уже отсортированы и преобразованы.

Выполняются следующие операции с данными:

* Извлечение самих данных – вся информация от источников переходит в отдельную Базу данных и приводится к единому
  формату
* Преобразование данных – информация подготавливается для хранения в оптимальной форме
* Загрузка и обновление данных – данные помещаются в хранилище
* Анализ данных
* Результаты анализа данных

![img.png](images/dwh_img.png)

Грубо говоря - данные приводятся к какой-то модели

### Data Lake

**Data Lake** - хранилище, где хранятся все необработанные данные в исходном формате без преобразования. Каждый элемент
в Data Lake обозначен уникальным идентификатором и набором тегов метаданных. Данные могут быть неструктурированными,
полуструктурированными
или структурированными, они преобразуются только при запросе на использование.

Оба подхода используют разные процессы для управления данными. Data Warehouse использует метод *ETL* – Extract,
Transform
и Load. В свою очередь, Data Lake использует *ELT* — Extract, Load и Transform.

### ETL и ELT

*ETL* использует промежуточный сервер для преобразования данных и только потом загружает их в хранилище Data Warehouse.
Такой подход применяется для небольшого количества данных и вычислительных преобразований.

*ELT* работает с большими объемами данных и сразу грузит их в Базу данных, их преобразование происходит уже в целевой
системе. Скорость загрузки данных никогда не зависит от их размера. Из минусов, ELT сложнее внедрить в систему в отличие
от ETL, так как для внедрения и поддержки ELT требуются нишевые знания.

### Сервисы Hadoop

#### Yarn

Является аналогом spark standalone scheduler, который запускается по умолчанию и входит в spark.

*Yarn* - resource manager, управляет запуском контейнеров, в которых уже и работают spark приложения

![img.png](images/yarn_img.png)

Приходит задача на запуск приложения. Resource manager поднимает первый контейнер - Application master.
В нём запускается первый instance приложения, который идёт к resource manager-у и запрашивает уже ресурсы на исполнение.
Например - ещё 3 контейнера. Тот опрашивает node manager-ы в поисках свободных ресурсов.
Application master получив список нод обращается к node manager-м уже за ресурсами и там тоже запускаются экземпляры
нашего приложения.

Note:
spark executor запускается в yarn контейнере, а application master контейнер уже содержит драйвер - своего рода
контроллер.
yarn executor - это дочерний процесс.

![img.png](images/driver_img.png)

Разделение ресурсов происходит по очередям. Каждое приложение попадает в свою очередь.

#### Apache Oozie

*Apache Oozie* — это планировщик рабочих процессов для Hadoop. Это система, которая запускает рабочий процесс зависимых
заданий. Здесь пользователям разрешается создавать направленные циклические графы рабочих процессов, которые можно
запускать параллельно и последовательно в Hadoop. Оркестратор, в котором через xml описывается pipeline

#### MapReduce

![img.png](images/map_reduce_img.png)

#### ZooKeeper

*ZooKeeper* - каталог информации о кластере для сервисов

### HDFS - проблема мелких файлов

Namenode, которая содержит информацию о расположении всех файлов, хранится в ОЗУ.
Рассмотрим в качестве примера файл в 10мб.
Его можно хранить как:

~~~
blck_1 10 mb
~~~

или:

~~~
blck_1 1 mb
...
black_10 1 mb
~~~

Второй способ плох, так как namenode хранит данные в ОЗУ, а хранение 1й записи дешевле 10и.

**Рекомендуемый размер блока в HDFS - 128 мб**

## Python vs Scala

Как работает Pyspark? Когда запускаем задачу на Spark - поднимаются JVM процессы. Ч
то происходит при обращении к RDD? Из JVM процессов через библиотеку PY4J будут осуществляться вызовы на python
интерпретаторах,
которые установлены на каждой ноде кластера и они будут работать совместно. Так как RDD API низкроуровневое -
по сути мы выполняем на каждом шаге какую-то функцию.
Соответственно, они будут выполняться в Python среде,
а основной код задачи в JVM => на каждом шаге будет происходить переключение контекста, по сути - пересылка данных
между python интерпретатором и JVM машиной.
Это очень дорого.

Python:

* Большое сообщество и много библиотек
* Низкая скорость работы RDD API:
* UDF - низкая скорость работы в DF API и Streaming API. (В большинстве своём)

Scala:

* Нестабильные ноутбуки, spark-shell и spark-sql работают не очень стабильно с большими объёмами, неудобно.
* Высокая скорость Spark API
* Scala UDF можно использовать в Pyspark: их можно написать, скомпилировать в jar, подключить и использовать


## RDD API

Количество executor на node? Стандартно - по количеству ядер. 
Каждый executor - отдельный процесс(один процесс, на одном ядре => одна задача).

Spark context, Spark session? Абстракция, которую возвращает библиотека для работы со спарком.

Driver разбивает код на стадии и они выстраиваются в граф. Spark решает задачу mapping этих стадий на железо, которое у нас есть.

![img.png](images/rdd.png)

![img.png](images/shuffling.png)


## Dataframe

Быстрее чем RDD, за счёт sql оптимизаций.

## Lineage

Возможность посмотреть как из Dataset A получился Dataset B.

.explain()
.toDebugString

## Партиционирование

![img.png](img.png)