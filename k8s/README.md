# Абстракции

* Namespace — пространство имен. Объекты могут взаимодействовать, только если находятся в одном namespace. С помощью
  namespace возможно развернуть несколько виртуальных кластеров на одном физическом.
* Pod — минимальный юнит развертывания. Это объединение из нескольких контейнеров. Даже если нам нужно запустить всего
  один - кубер поднимет минимум 2.
* ReplicaSet — контроллер, позволяющий создать набор одинаковых подов и работать с ними, как с единой сущностью.
  Поддерживает нужное количество реплик, при необходимости создавая новые поды или убивая старые. На самом деле в
  большинстве случаев вы не будете работать с ReplicaSet напрямую — для этого есть Deployment.
* Deployment — контроллер развертывания, являющийся абстракцией более высокого уровня над ReplicaSet'ом. Добавляет
  возможность обновления управляемых подов.
* Service — отвечает за сетевое взаимодействие группы подов. В системе обычно существует несколько экземляров одного
  микросервиса, соответственно каждый из них имеет свой IP-адрес. Количество подов может изменяться, следовательно набор
  адресов также не постоянен. Другим частям системы для доступа к рассматриваемым подам нужен какой-то статичный адрес,
  который Service и предоставляет.
* Ingress сам по себе не сервис. Он стоит перед несколькими сервисами и действует как «интеллектуальный маршрутизатор»
  или точка вхождения в кластер.
* ConfigMap — объект с произвольными конфигурациями, которые могут, например, быть переданы в контейнеры через
  переменные среды.
* Secret — объект с некой конфиденциальной информацией. Секреты могут быть файлами (№ SSL-сертификатами), которые
  монтируются к контейнеру, либо же base64-закодированными строками, передающимися через те же переменные среды.
* HorizontalPodAutoscaler — объект, предназначенный для автоматического изменения количества подов в зависимости от их
  загруженности.

<img src="images/k8s_pod.png" width="570" height="340" />

## Deployment

### Health Check

## Service

*Service - это про DNS запись на ноде*

<img src="images/service_access_from_node.png" width="800" height="200" />

Curl-м и попадаем в разные Pod-ы. Внутри pod-в мы можем обращаться к другим используя dns имя.

### ClusterIP

ClusterIP — сервис Kubernetes по умолчанию. Он обеспечивает сервис внутри кластера, к которому могут обращаться другие
приложения внутри кластера. Внешнего доступа нет.

<img src="images/service_clusterip.png" width="500" height="450" />


Однако можно открыть порт:

~~~bash
kubectl port-forward service/{service-name} 10000:80 
# 10000 - порт на localhost
# 80 - port в service на который надо принимать трафик
~~~

Используется для внутрикластерной балансировки. Она подойдет, например, для организации взаимодействия отдельных групп
подов, расположенных в пределах одного кластера Kube. Организовать доступ к службе можно организовать двумя способами:
через DNS или при помощи переменных окружения.
Что касается переменных среды, то они устанавливаются при запуске нового пода через инструкцию service-name. Вам могут
понадобиться переменные PORT и SERVICE_HOST, а вот инструкции для их установки:
```
service-name_PORT
service-name_SERVICE_HOST
```

### NodePort

Сервис NodePort — самый примитивный способ направить внешний трафик в сервис. NodePort, как следует из названия,
открывает указанный порт для всех Nodes (виртуальных машин), и трафик на этот порт перенаправляется сервису.

<img src="images/service_nodeport.png" width="500" height="450" />


По сути, сервис NodePort имеет два отличия от обычного сервиса ClusterIP. Во-первых, тип NodePort. Существует
дополнительный порт, называемый nodePort, который указывает, какой порт открыть на узлах. Если мы не укажем этот порт,
он выберет случайный. В большинстве случаев дайте Kubernetes самому выбрать порт.

*Хорошо подходит для публикации не HTTP трафика. Тот же PG 5432 или Rabbit*

Метод имеет множество недостатков:

* На порт садится только один сервис
* Доступны только порты 30000–32767
* Если IP-адрес узла/виртуальной машины изменяется, придется разбираться

Можно использовать для локальной отладки

### LoadBalancer

Сервис LoadBalancer — стандартный способ предоставления сервиса в интернете. На GKE он развернет Network Load Balancer,
который предоставит IP адрес. Этот IP адрес будет направлять весь трафик на сервис.
Используется для внешних облачных балансировщиков, таких как Google Cloud, которые имеют своего провайдера. Сервис будет
доступен через внешний балансировщик вашего провайдера, при этом создаются NodePort с портами, куда будет приходить
трафик от провайдера и ClusterIP.

<img src="images/service_loadbalancer.png" width="480" height="450" />

Если вы хотите раскрыть сервис напрямую, это метод по умолчанию. Весь трафик указанного порта будет направлен на сервис.
Нет фильтрации, нет маршрутизации и т.д. Это означает, что мы можем направить на сервис такие виды трафика как HTTP,
TCP, UDP, Websockets, gRPC и тому подобное.

! Но есть один недостаток. Каждому сервису, который мы раскрываем с помощью LoadBalancer, нужен свой IP-адрес, что может
влететь в копеечку.

### ExternalIPs

Тоже что и NodePort, только открывает не порты, а ip.
Трафик приходящий на некоторый ip будет отправлять в наши Pod-ы

<img src="images/service_externalIp.png" width="480" height="450" />

### Headless

Для баз данных. Создаст DNS запись на каждой ноде.
С ним можно разделять трафик направляя, к примеру, чтение на одну ноду, а запись на другую.

<img src="images/service_headless.png" width="480" height="450" />

## Ingress

Сам по себе не сервис. Он стоит перед несколькими сервисами и действует как «интеллектуальный маршрутизатор» или точка
вхождения в кластер.

<img src="images/ingress.png" width="520" height="450" />

**Ingress** — свод правил, в которых описано, как внешний трафик получает доступ к сервисам в кластере(манифест).

**Ingress-контроллер** — pod, который реализует правила, описанные в Ingress. По сути, это
приложение-контроллер/балансировщик, который работает в кластере. Nginx, Haproxy...

## k8s dashboard

https://alnotes.ru/DevOps/kubernetes/dashboard

~~~
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
kubectl proxy
~~~

UI: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login

kubectl get secret/admin-user -o jsonpath='{.data.token}' -n kubernetes-dashboard | base64 --decode